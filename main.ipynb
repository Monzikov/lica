{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcdf4cc",
   "metadata": {},
   "source": [
    "MAI 2025 GAN Deepfake Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a760c2e",
   "metadata": {},
   "source": [
    "Задача с deepfake показалась максимально захватывающей, вызывающей и сложной, которая должна пополнить копилку опыта и навыков.\n",
    "\n",
    "Было принято решение перенять опыт - опробовать в деле текущие популярные, где-то даже \"нашумевшие\" открытые репозитории предлагающие deepfake функционал. \n",
    "\n",
    "DeepFaceLab - самое популярное, интересное решение, но уже не поддерживается, последнее обновление датировано ноябрем 2024. Код стройный, но запустить так и не удалось. Пазл не собрался.\n",
    "\n",
    "FaceSwap - имеет относительно свежие обновления, но как показало знакомство рабочим по умолчанию не является, проблемы виртуальных окружений и совместимости пакетов python не дали далко продвинуться.\n",
    "Был даже найден установщик последнего релиза под Windows, но и его работоспособность напрямую зависит от версии установленного Python, его пакетов библиотек, которых уже нет в репозиториях.\n",
    "\n",
    "SimSwap - показался самым запутанным, и самым не подходящим по организации взаимодействия, но это единственное решение, которое смогло показать результат, В релизных сборках явно присутствуют деффекты, наример как ссылки на локальные диски, а организация кода оставляет желать лучшего. С большими усиллиями и дебагингом всё же удалось получить хоть какой-то результат, но об \n",
    "подробне немного позже.\n",
    "\n",
    "First Order Motion Mode - последнее обновление датировано ноябрем 2024. Самое скромное решение из популярных. Минимум документации, максимум боли, и нулевой результат. Все попытки реанимитровать его потерпели неудачу. \n",
    "\n",
    "Roop - по заявлениям разработчика, это всё ещё рабочий инструмент, но он тоже заброшен в марте 2024. И тоже потребовал уникального набора библиотек, которые просто конфилоиктовали друг с другом по кругу.\n",
    "\n",
    "Решения использующие Python 3.6 и ниже, сразу отсекались, так как только на установку старой версии Pyhton можно потратить всё отведенное на проект время."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050397e",
   "metadata": {},
   "source": [
    "Первое с чем сталлкивается исследователь это боль работы с GPU. Вроде уже все понимают, что без GPU в этом процессе никуда, и есть две основные бибилиотеки tensorflow и torch, которые имеют сбори использующие графический ускоритель.\n",
    "\n",
    "Установив их посделние версии, и опробовав использовать графические ядра мы сталкиваемся с данностью - CUDA ядра не видны и не применяются библиотеками.\n",
    "Изучив внимательно примечания на сайте tensorflow, была получена информация о том, что сборка использующая GPU доступна только в Linux или WSL2.\n",
    "\n",
    "Выводы сделаны, мы получили прямое указание работать в WSL, это с недавних пор стало удобно выполнять из VS Code. \n",
    "Запускаем WSL ставим Ubuntu. Ставим драйвера nvidia, накатываем torch и tensorflow. Регистрируем разые python окружения. И только это хоть как-то спасает ситуацию.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a28633",
   "metadata": {},
   "source": [
    "Забегая немного вперёд, скажу, что работая над проектом получилось максимально собрать незабываемый и непередаваемый опыт по получению доступа к своему оборудованию под нужды исследования."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9e4b6c",
   "metadata": {},
   "source": [
    "**Глава 1. Что получилось - результат 1. SimSwap**\n",
    "\n",
    "Из всего многообразия исследованных инструментов, на актуальном стеке Python >=3.12 удалось запустить только SimSwap.\n",
    "Не без трудностей, не без дебага, с исправлением конфликтов бибилиотек, с поиском предварительно отренированных весов.\n",
    "Удалось получить скромный результат по замене лица в видео файле. Исследованы и взяты на вооружение подходы к обработке видео кадров через ffmpg.\n",
    "\n",
    "Пример полученного результата размещен в каталоге data/results/simswap\n",
    "\n",
    "Попытки дообучить модель на своём датасете потерпели крах, как и попытки обучить модель с нуля на том же датасете.\n",
    "\n",
    "О промышленном или публичном применении полученной конфигурации говорить не приходится. Единственный относительно рабочий вариант - собрать докер образ, но это не цель нашего проекта."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81720b3",
   "metadata": {},
   "source": [
    "В попытках хоть как-то приблизиться к системе создания deepfakе,было принято следующее решение - попробовать реализовать модель для проекта самостоятельно.\n",
    "\n",
    "Первым шагом намечено получение устойчивого результата по замене лиц. Опираясь на SimSwap попытаться собрать приемлемый результат.\n",
    "\n",
    "Получив устойчивый резульатат для одного изображения, можно будет перейти к работе с видео."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c4a15",
   "metadata": {},
   "source": [
    "Перавым делом был собран датасет из личного альбома. \n",
    "Получены 387 реальных изображений одного лица, в разных ракурсах и в разной сложности узнаваимости. 256x256 и 512х512 в каталоге data/train/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e06022d",
   "metadata": {},
   "source": [
    "Импортируем необходимые модули."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "727d0adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec16f73",
   "metadata": {},
   "source": [
    "Пробуем с малого. Небольшой энкодер в 4 слоя и RELU между ними"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4157ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(3, 64, 5, stride=2, padding=2),  # 256x256 → 128x128\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 5, stride=2, padding=2), # 128x128 → 64x64\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, 5, stride=2, padding=2), # 64x64 → 32x32\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d(1)  # Глобальный пулинг\n",
    "        )\n",
    "        self.fc = nn.Linear(256, 512)  # ID-вектор\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65914f0",
   "metadata": {},
   "source": [
    "Соответствующий энкодеру декодер."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "90753262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv2d(256 + 512, 256, 3, padding=1),  # Объединяем features + ID\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # 32x32 → 64x64\n",
    "            nn.Conv2d(256, 128, 3, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # 64x64 → 128x128\n",
    "            nn.Conv2d(128, 64, 3, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2),  # 128x128 → 256x256\n",
    "            nn.Conv2d(64, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82d97b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swapper(nn.Module):\n",
    "    def __init__(self, latent_dim=512):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2, latent_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(latent_dim, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, source_id, target_features):\n",
    "        # source_id: [B, 512], target_features: [B, C, H, W]\n",
    "        B, C, H, W = target_features.shape\n",
    "        id_expanded = source_id.view(B, -1, 1, 1).expand(-1, -1, H, W)\n",
    "        return torch.cat([target_features, id_expanded], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0942127",
   "metadata": {},
   "source": [
    "Ну и собственно сама магия. Извлечение вектора, извлечение фич, объединение и отправка в декодер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bf4e29f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSwap(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.id_encoder = IDEncoder()\n",
    "        self.feature_extractor = IDEncoder()  # Для извлечения признаков позы/мимики\n",
    "        self.swapper = Swapper()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "    def forward(self, source_img, target_img):\n",
    "        # Извлекаем ID исходного лица\n",
    "        source_id = self.id_encoder(source_img)\n",
    "        \n",
    "        # Извлекаем признаки позы/мимики из целевого изображения\n",
    "        target_features = self.feature_extractor.layers[:-1](target_img)  # Без FC-слоя\n",
    "        \n",
    "        # Объединяем ID и признаки\n",
    "        swapped = self.swapper(source_id, target_features)\n",
    "        \n",
    "        # Генерируем результат\n",
    "        return self.decoder(swapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f90f4",
   "metadata": {},
   "source": [
    "Определяем функцию потерь для предстоящего обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89a51d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCalculator:\n",
    "    def __init__(self, device):\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.vgg = models.vgg16(pretrained=True).features[:16].to(device).eval()\n",
    "        for param in self.vgg.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def perceptual_loss(self, x, y):\n",
    "        return torch.mean(torch.abs(self.vgg(x) - self.vgg(y)))\n",
    "\n",
    "    def compute(self, real, fake, swapped):\n",
    "        # Reconstruction loss\n",
    "        loss_rec = self.mse(real, fake)\n",
    "        \n",
    "        # Perceptual loss\n",
    "        loss_perc = self.perceptual_loss(real, fake)\n",
    "        \n",
    "        # Adversarial loss (опционально)\n",
    "        # Можно добавить GAN-компонент\n",
    "        \n",
    "        return 0.7 * loss_rec + 0.3 * loss_perc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca113a",
   "metadata": {},
   "source": [
    "Декларация процесса обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "239b846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, epochs=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    criterion = LossCalculator(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for source, target in dataloader:\n",
    "            source, target = source.to(device), target.to(device)\n",
    "            \n",
    "            # Генерация\n",
    "            swapped = model(source, target)\n",
    "            \n",
    "            # Вычисление потерь\n",
    "            loss = criterion.compute(target, swapped, source)\n",
    "            \n",
    "            # Обновление весов\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80fac5b3",
   "metadata": {},
   "source": [
    "немного удобств для последующего применения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "906a6f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_face(model, source_img, target_frame):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        swapped = model(source_img, target_frame)\n",
    "    return swapped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8694f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, img_size=256, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Загружает изображение, обнаруживает лицо, выравнивает и нормализует.\n",
    "    Возвращает тензор [1, 3, H, W] в диапазоне [-1, 1].\n",
    "    \"\"\"\n",
    "    # Загрузка изображения и конвертация в RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Обнаружение лица (упрощенная версия - для реального использования лучше MTCNN)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "    \n",
    "    if len(faces) == 0:\n",
    "        raise ValueError(\"Лицо не обнаружено на изображении!\")\n",
    "    \n",
    "    # Вырезаем лицо\n",
    "    (x, y, w, h) = faces[0]\n",
    "    face = image[y:y+h, x:x+w]\n",
    "    \n",
    "    # Ресайз и нормализация\n",
    "    face = cv2.resize(face, (img_size, img_size))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    ])\n",
    "    face_tensor = transform(face).unsqueeze(0).to(device)\n",
    "    \n",
    "    return face_tensor\n",
    "\n",
    "def save_image(tensor, path):\n",
    "    \"\"\"Сохраняет тензор [1, 3, H, W] в диапазоне [-1, 1] как изображение\"\"\"\n",
    "    tensor = tensor.squeeze(0).cpu().detach()\n",
    "    tensor = tensor * 0.5 + 0.5  # [-1, 1] -> [0, 1]\n",
    "    transforms.ToPILImage()(tensor).save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa1a99",
   "metadata": {},
   "source": [
    "Класс обертка для датасета с лицами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdf35bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, source_dir, target_dir, img_size=256):\n",
    "        self.source_paths = sorted([os.path.join(source_dir, f) for f in os.listdir(source_dir)])\n",
    "        self.target_paths = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir)])\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        ])\n",
    "        self.img_size = img_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.source_paths), len(self.target_paths))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_img = self._load_image(self.source_paths[idx])\n",
    "        tgt_img = self._load_image(self.target_paths[idx])\n",
    "        return src_img, tgt_img\n",
    "\n",
    "    def _load_image(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (self.img_size, self.img_size))\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d65119",
   "metadata": {},
   "source": [
    "Создаем dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a0878a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FaceDataset(\"data/train/source\", \"data/train/target_256\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bd051b",
   "metadata": {},
   "source": [
    "Запуск процесса обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5523b824",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "model = SimpleSwap().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cbcd288f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.0443\n",
      "Epoch 1, Loss: 0.0275\n",
      "Epoch 2, Loss: 0.0355\n",
      "Epoch 3, Loss: 0.0272\n",
      "Epoch 4, Loss: 0.0302\n",
      "Epoch 5, Loss: 0.0131\n",
      "Epoch 6, Loss: 0.0373\n",
      "Epoch 7, Loss: 0.0187\n",
      "Epoch 8, Loss: 0.0328\n",
      "Epoch 9, Loss: 0.0172\n",
      "Epoch 10, Loss: 0.0220\n",
      "Epoch 11, Loss: 0.0190\n",
      "Epoch 12, Loss: 0.0146\n",
      "Epoch 13, Loss: 0.0107\n",
      "Epoch 14, Loss: 0.0123\n",
      "Epoch 15, Loss: 0.0182\n",
      "Epoch 16, Loss: 0.0097\n",
      "Epoch 17, Loss: 0.0249\n",
      "Epoch 18, Loss: 0.0112\n",
      "Epoch 19, Loss: 0.0103\n",
      "Epoch 20, Loss: 0.0152\n",
      "Epoch 21, Loss: 0.0081\n",
      "Epoch 22, Loss: 0.0092\n",
      "Epoch 23, Loss: 0.0153\n",
      "Epoch 24, Loss: 0.0087\n",
      "Epoch 25, Loss: 0.0120\n",
      "Epoch 26, Loss: 0.0091\n",
      "Epoch 27, Loss: 0.0070\n",
      "Epoch 28, Loss: 0.0137\n",
      "Epoch 29, Loss: 0.0077\n",
      "Epoch 30, Loss: 0.0118\n",
      "Epoch 31, Loss: 0.0100\n",
      "Epoch 32, Loss: 0.0076\n",
      "Epoch 33, Loss: 0.0263\n",
      "Epoch 34, Loss: 0.0058\n",
      "Epoch 35, Loss: 0.0082\n",
      "Epoch 36, Loss: 0.0429\n",
      "Epoch 37, Loss: 0.0068\n",
      "Epoch 38, Loss: 0.0069\n",
      "Epoch 39, Loss: 0.0080\n",
      "Epoch 40, Loss: 0.0047\n",
      "Epoch 41, Loss: 0.0054\n",
      "Epoch 42, Loss: 0.0058\n",
      "Epoch 43, Loss: 0.0061\n",
      "Epoch 44, Loss: 0.0084\n",
      "Epoch 45, Loss: 0.0167\n",
      "Epoch 46, Loss: 0.0058\n",
      "Epoch 47, Loss: 0.0032\n",
      "Epoch 48, Loss: 0.0058\n",
      "Epoch 49, Loss: 0.0026\n",
      "Epoch 50, Loss: 0.0040\n",
      "Epoch 51, Loss: 0.0058\n",
      "Epoch 52, Loss: 0.0027\n",
      "Epoch 53, Loss: 0.0035\n",
      "Epoch 54, Loss: 0.0054\n",
      "Epoch 55, Loss: 0.0029\n",
      "Epoch 56, Loss: 0.0035\n",
      "Epoch 57, Loss: 0.0043\n",
      "Epoch 58, Loss: 0.0065\n",
      "Epoch 59, Loss: 0.0036\n",
      "Epoch 60, Loss: 0.0054\n",
      "Epoch 61, Loss: 0.0036\n",
      "Epoch 62, Loss: 0.0049\n",
      "Epoch 63, Loss: 0.0041\n",
      "Epoch 64, Loss: 0.0031\n",
      "Epoch 65, Loss: 0.0066\n",
      "Epoch 66, Loss: 0.0034\n",
      "Epoch 67, Loss: 0.0031\n",
      "Epoch 68, Loss: 0.0075\n",
      "Epoch 69, Loss: 0.0031\n",
      "Epoch 70, Loss: 0.0032\n",
      "Epoch 71, Loss: 0.0060\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m100\u001b[39m):\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m batch_src, batch_tgt \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         batch_src, batch_tgt = \u001b[43mbatch_src\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, batch_tgt.cuda()\n\u001b[32m      5\u001b[39m         \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m      6\u001b[39m         swapped = model(batch_src, batch_tgt)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for batch_src, batch_tgt in dataloader:\n",
    "        batch_src, batch_tgt = batch_src.cuda(), batch_tgt.cuda()\n",
    "        \n",
    "        # Forward pass\n",
    "        swapped = model(batch_src, batch_tgt)\n",
    "        \n",
    "        # Loss calculation (например, MSE + Perceptual Loss)\n",
    "        loss = torch.nn.functional.mse_loss(swapped, batch_tgt)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "    torch.save(model.state_dict(), f\"data/train/weights/simpleswap_99_{epoch}.pth\")  # Сохраняем веса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee02051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# когда надо сохранить какой-либо набор значений весов\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'epoch': 99,\n",
    "    'loss': 0.0059,\n",
    "}, \"simpleswap_99_fix.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928d4217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# когда надо загрузить в модель какой-либо набор значений весов\n",
    "checkpoint = torch.load(\"simpleswap_99.pth\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d028f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleSwap(\n",
       "  (id_encoder): IDEncoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (8): ReLU()\n",
       "      (9): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (feature_extractor): IDEncoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
       "      (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (8): ReLU()\n",
       "      (9): AdaptiveAvgPool2d(output_size=1)\n",
       "    )\n",
       "    (fc): Linear(in_features=256, out_features=512, bias=True)\n",
       "  )\n",
       "  (swapper): Swapper(\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (layers): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (2): ReLU()\n",
       "      (3): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (4): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (6): ReLU()\n",
       "      (7): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (8): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      (10): ReLU()\n",
       "      (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "      (12): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): Tanh()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотреть модель\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e1d25f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"data/src/lic.jpg\"\n",
    "trg = \"data/src/target.jpg\"\n",
    "res = \"data/result/r_03.jpg\"\n",
    "#wgh = \"/mnt/c/code/GENS/lica/data/weights/latest_net_G.pth\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = SimpleSwap().to(device)\n",
    "# model.load_state_dict(torch.load(wgh))\n",
    "\n",
    "# # Загрузка изображений\n",
    "source_img = load_image(src, device=device)\n",
    "target_frame = load_image(trg, device=device)\n",
    "\n",
    "# Замена лица\n",
    "result = swap_face(model, source_img, target_frame)\n",
    "\n",
    "# Сохранение результата\n",
    "save_image(result, res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6b338cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"data/src/lic.jpg\"\n",
    "trg = \"data/src/target.jpg\"\n",
    "source_img = load_image(src, device=device)\n",
    "target_frame = load_image(trg, device=device)\n",
    "res = \"data/result/r_032.jpg\"\n",
    "with torch.no_grad():\n",
    "    swapped = model(source_img, target_frame)\n",
    "    save_image(swapped, res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29d7bdf",
   "metadata": {},
   "source": [
    "**Вывод**\n",
    "\n",
    "Портить видео и фотографии мы научились, осталось дело за малым - достичь совершенства.\n",
    "Показывать данное решение на общую публику не рекомендовано.\n",
    "\n",
    "Следующий шаг - попытка применить GAN хотя бы на уровне порчи фортографий."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
